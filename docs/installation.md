# h3abionet/chipimputation_evaluate_chips: Installation

To start using the h3achipimputation pipeline, follow the steps below:

1. [Install Nextflow](#1-install-nextflow)
2. [Install the pipeline](#2-install-the-pipeline)
    * [Automatic](#21-automatic)
    * [Offline](#22-offline)
    * [Development](#23-development)
3. [Pipeline configuration](#3-pipeline-configuration)
    * [Software deps: Singularity](#31-software-deps-singularity)
    * [Configuration profiles](#33-configuration-profiles)


## 1) Install NextFlow
Nextflow runs on most POSIX systems (Linux, Mac OSX etc). It can be installed by running the following commands:

```bash
# Make sure that Java v8+ is installed:
java -version

# Install Nextflow
curl -fsSL get.nextflow.io | bash

# Add Nextflow binary to your PATH:
mkdir -p ~/bin 
mv nextflow ~/bin/
# OR system-wide installation:
# sudo mv nextflow /usr/local/bin
```

See [nextflow.io](https://www.nextflow.io/) for further instructions on how to install and configure Nextflow.

## 2) Install the pipeline

#### 2.1) Automatic
This pipeline itself needs no installation - NextFlow will automatically fetch it from GitHub if `h3abionet/chipimputation` is specified as the pipeline name with the ecorrect `evaluate_chips` branch with: `h3abionet/chipimputation_evaluate_chips`


#### 2.2) Offline
The above method requires an internet connection so that Nextflow can download the pipeline files. If you're running on a system that has no internet connection, you'll need to download and transfer the pipeline files manually:

```bash
git clone https://github.com/h3abionet/chipimputation_evaluate_chips.git
```

To stop nextflow from looking for updates online, you can tell it to run in offline mode by specifying the following environment variable in your ~/.bashrc file:

```bash
export NXF_OFFLINE='TRUE'
```

#### 2.3) Development

If you would like to make changes to the pipeline, it's best to make a fork on GitHub and then clone the files. Once cloned you can run the pipeline directly as above.


## 3) Pipeline configuration

By default, the pipeline runs with the `test` configuration profile corresponding to the argument `-profile test`, unless a configuration file is provided. This uses a number of test defaults for process requirements and is suitable for running on a simple (if powerful!) basic server. You can see this configuration in [`conf/test.config`](../conf/test.config).  

#### 3.1) Configuration file
Be warned of important parameters about this default configuration:

1. The reference panel(s): `ref_panels`
    * This pipeline can run on multiple reference panels. Each reference panel will be used for a separate run.
    * All files are provided in chromosomes using string interpollation `%s`
    * Example: 
        ```
        ref_panels {
            m3vcfFile   = "refPanelTestdata_chr%s.m3vcf.gz" // M3VCF file format, generated by minimac3. Used by minimac4 during imputation.
            vcfFile 	= "refPanelTestdata_chr%s.vcf.gz" // Used by Eagle during phasing, and for frequency comparison.
            sampleFile  = "refPanelTestdata.sample" // Sample file containing sample deetails such ID, group, population, sex
        }
        If multiple reference panels:
        testRefPanel1 {
            m3vcfFile   = "refPanelTestdata_chr%s.m3vcf.gz" // M3VCF file format, generated by minimac3. Used by minimac4 during imputation.
            vcfFile 	= "refPanelTestdata_chr%s.vcf.gz" // Used by Eagle during phasing, and for frequency comparison.
            sampleFile  = "refPanelTestdata.sample" // Sample file containing sample deetails such ID, group, population, sex
        }
        ```
        
2. The target dataset `target_datasets`
    * This is basically the dataset to impute.
    * This pipeline can run on multiple target datasets. Each target dataset will be used for a separate run.
    * Example:
    
3. Array files: `tagSNPs_files`
    * Description: 
    * Example:
4. Genetic map for eagle2: `eagle_genetic_map`
5. Fasta reference genome used for QC reference_genome: `reference_genome`


#### 3.2) Software deps: Singularity
If you're not able to use Docker then [Singularity](http://singularity.lbl.gov/) is a great alternative.
The process is very similar: running the pipeline with the option `-profile test,singularity` tells Nextflow to enable singularity for this run. An image containing all of the software requirements will be automatically fetched and used from singularity hub.

If running offline with Singularity, you'll need to download and transfer the Singularity image first:

```bash
singularity pull --name h3abionet-chipimputation-minimac4.simg docker://quay.io/h3abionet_org/impute2:v3
```

Once transferred, use `-with-singularity` and specify the path to the image file:

```bash
nextflow run h3abionet/chipimputation -with-singularity h3abionet-chipimputation-minimac4.simg
```

Remember to pull updated versions of the singularity image if you update the pipeline.

* See the [nextflow docs](https://www.nextflow.io/docs/latest/executor.html) for information about running with other hardware backends. Most job scheduler systems are natively supported.

#### 3.3) Configuration profiles

### `-profile`
Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments. Note that multiple profiles can be loaded, for example: `-profile standard,docker` - the order of arguments is important!

* `standard`
    * The default profile, used if `-profile` is not specified at all.
    * Runs locally and expects all software to be installed and available on the `PATH`.
* `singularity`
    * A generic configuration profile to be used with [Singularity](http://singularity.lbl.gov/)
    * Pulls software from quay.io
* `test`
    * A profile with a complete configuration for automated testing
    * Includes links to test data so needs no other parameters
* `none`
    * No configuration at all. Useful if you want to build your own config from scratch and want to avoid loading in the default `base` config profile (not recommended).
